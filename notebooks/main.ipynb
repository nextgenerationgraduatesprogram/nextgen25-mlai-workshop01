{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch utils directory from GitHub repo (workaround for using the utils in colab)\n",
    "!curl -sL https://github.com/nextgenerationgraduatesprogram/nextgen25-mlai-workshop01/archive/refs/heads/main.zip -o repo.zip\n",
    "!unzip -q repo.zip \"nextgen25-mlai-workshop01-main/utils/*\" -d .\n",
    "!rm -rf ./utils\n",
    "!mv nextgen25-mlai-workshop01-main/utils ./utils -f\n",
    "!rm -rf nextgen25-mlai-workshop01-main repo.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565dcb4d",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    ü§ñ MLAI Workshop #01\n",
    "</h1>\n",
    "\n",
    "Welcome to the NextGen 2025 Machine Learning and Artificial Intelligence (MLAI) Workshop Series! Across many machine learning courses and tutorial I've found the focus is often on how to use a specific architecture, implement a certain model, or design a pipeline - and that's useful! But I find its much less common for those resources to step back and ask:\n",
    "- What is does a model represent?\n",
    "- What does it mean to learn from data?\n",
    "- Why does this work ‚Äî and when does it fail?\n",
    "\n",
    "The goal of this series of workshops isn‚Äôt just to show you how to use machine learning models - it's to explore these questions - to facilitate develop a deeper understanding of the fundamentals that underpin how and why machine learning works. To achieve this, we'll take a step back from specific tools of libraries and instead focus on core concepts such as functions, hypothesis spaces, data, optimization, and generalization.\n",
    "\n",
    "üí¨ *Question for the audience! Who here works with machine learning regularly? Who here focuses on the theory behind it?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b21d3c",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    üóìÔ∏è Agenda\n",
    "</h2>\n",
    "\n",
    "1. **What are functions?** Understanding functions as the backbone of machine learning ‚Äî mappings from input to output, and how they describe the world.\n",
    "2. **How do we represent functions?** Introduction to hypothesis spaces, parameterized models, and the role of structure and flexibility in modeling - work through some example to demonstrate these ideas.\n",
    "\n",
    "üßÉ *Break*\n",
    "\n",
    "3. **How do we calibrate functions?** Exploring data-driven learning through visualization of loss functions, optimization, and searching for good approximations of the target function.\n",
    "\n",
    "üßÉ *Break*\n",
    "\n",
    "3. **How useful is this function?** Generalization, approximation vs. estimation, and how our choices impact model performance on seen and unseen data.\n",
    "4. **Modeling in practice** Put this theory into practice through a group activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d617775",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49301159",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Section 1: Functions\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4bda5",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    üìà Section 1A: What are functions and why should I care?\n",
    "</h3>\n",
    "\n",
    "Functions describe the world. Functions allow us to express relationships between objects and are critical in virtually every scientific, engineering, and data-driven domain. From physical phenomena such as the dynamics of protein folding and the behavior of fluids, to abstract processes like decision-making and natural language understanding - functions describe the world.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/nextgenerationgraduatesprogram/nextgen25-mlai-workshop01/raw/main/media/notebook/GraphCast_Forecast.jpg\" height=\"400\"/>\n",
    "    <p><em>Figure 1. GraphCast is a Google DeepModel model for faster and more accurate global weather forecasting.</em></p>\n",
    "</div>\n",
    "\n",
    "So what is a function? Generally put, a function $f$ is an object that maps an input space $\\mathcal{X}$ to an output space $\\mathcal{Y}$:\n",
    "\n",
    "\\begin{align*}\n",
    "f: \\mathcal{X} \\to \\mathcal{Y} \\tag{1.1}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6e95c",
   "metadata": {},
   "source": [
    "For example, the linear equation $f(x) = \\theta_{1} x + \\theta_{2}$ is a function that maps a given input $x \\in \\mathbb{R}$ to an output $y \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d690e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some function\n",
    "def f(x):\n",
    "    return -7.13 * x + 0.51\n",
    "\n",
    "# map the input to the output\n",
    "x = 1.5\n",
    "y = f(x)\n",
    "\n",
    "print(f\"f: {x:.2f} -> {y:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a46c5",
   "metadata": {},
   "source": [
    "More generally we can think nearly any process as a function. For example, we might imagine the birth and evolution of a hurricane, when and where the hurricane forms is going to depend on how the atmosphere evolves over time, this in turn depends on an unimagingable amount of factors. Whilst this is much more complex than a linear equation, at the risk of being reductionist, we can nonetheless consider it a function:\n",
    "\n",
    "\\begin{align*}\n",
    "    f^{*}: \\mathcal{X}^{*} \\to \\mathcal{Y}^{*} \\tag{1.2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $f^{*}$ represents the target function describing this relationship. (this may be a space of functions $\\mathcal{T}$)\n",
    "- $\\mathcal{X}^{*}$ represents the true current and/or historical atmospheric state.\n",
    "- $\\mathcal{Y}^{*}$ represents the true corresponding future atmospheric state.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/nextgenerationgraduatesprogram/nextgen25-mlai-workshop01/raw/main/media/notebook/GraphCast_Rollout.jpg\" height=\"400\"/>\n",
    "    <p><em>Figure 2. GraphCast takes as input the current atmospheric state and predicts the next state, this can be rolled out in an auto-regressive manner.</em></p>\n",
    "</div>\n",
    "\n",
    "This function $f^{*}$ encapsulates all the physics of the system ‚Äî from large-scale fluid dynamics to fine-grained thermodynamics, the impact of incident rays from the sun, the graviational effects of the moon, and even down to quantum-scale effects, - it reflects the true data generating process. While we oft cannot write down or compute $f^{*}$ directly, we assume it exists and is responsible for generating the weather patterns we observe. \n",
    "\n",
    "It's also important to note that $\\mathcal{X}^{*}$ and $\\mathcal{Y}^{*}$ represent the true states - this is typically different from the values we measure or observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f35e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = ... # <-- some complex function\n",
    "    return y\n",
    "\n",
    "x = ... # <-- input\n",
    "y = f(x) # <-- map input to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08240d14",
   "metadata": {},
   "source": [
    "üí¨ *Question for the audience! Are any people focused on applying machine learning in their research? What sort of function are they assuming exists and what are the inputs and outputs?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb89006",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    üìå Section 1B. Can we represent this function?\n",
    "</h3>\n",
    "\n",
    "So, there exists some target function $f^{*}$ that describes a process we care about - what next? Unfortunately, in most scenarios we don't have access to $f^{*}$ that describes the data generating process - if we did, we wouldn't be here today talking about machine learning. Instead, we often have to settle for defining some function $f$ that approximates $f^{*}$ well enough that it can be useful within certain scenarios. \n",
    "\n",
    "\\begin{align*}\n",
    "    f \\approx f^{*} \\tag{1.3}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We refer to the set of functions $f$ that approximate $f^{*}$ that meet this criterion as the target space $\\mathcal{T}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{T} = \\left\\{ f \\in \\mathcal{H} \\,|\\, \\mathcal{L}(f, f^{*}) \\leq \\epsilon \\text{ and f satisfies additional constraints} \\right\\} \\tag{1.4}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\mathcal{H}$ is the hypothesis space\n",
    "- $\\mathcal{L}(f, f^{*})$ is some function that evaluates the fit of the model\n",
    "- $\\epsilon$ is some evaluation metric threshold\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/nextgenerationgraduatesprogram/nextgen25-mlai-workshop01/raw/main/media/notebook/GraphCast_Error.png\" height=\"400\"/>\n",
    "    <p><em>Figure 3. GraphCast error distribution on a 12-hour forecast visualized across on mercator projection.</em></p>\n",
    "</div>\n",
    "\n",
    "For the example of atmospheric forecasting, you might consider $\\mathcal{T}$ as the set of solutions that forecasts to a specified degree of accuracy after a 12-hour rollout. This set of solutions we consider suitable is encapsulated in the choice of $\\mathcal{L}(f, f^{*})$ and $\\epsilon$. To visualize this idea and the next few cells lets head over to [Miro](https://miro.com/app/board/uXjVI44Nizk=/?share_link_id=794264052796).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5b39e",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    ü§î Hypothesis Space\n",
    "</h4>\n",
    "\n",
    "The hypothesis space $\\mathcal{H}$ defines the set of functions we're willing to consider for $f$, in this sense it represents our hypothesis functions for $f$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{H} = \\left\\{ f_{\\theta}: \\mathcal{X} \\to \\mathcal{Y} \\,|\\, f_{\\theta} \\text{ satisfies structural constraints} \\right\\} \\subseteq \\Theta \\tag{1.5}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $f_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{Y}$ represents a single function parameterized by parameters $\\theta$\n",
    "- $f_{\\theta} \\text{ satisfies structural constraints}$ represents our choice of modeling assumptions e.g. what sort of model we use\n",
    "- $\\Theta$ represents the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c4967",
   "metadata": {},
   "source": [
    "For example, we might have a modelling problem where we hypothesise some linear equation as a suitable solution, in this case we can define $\\mathcal{H}$ as:\n",
    "\n",
    "\\begin{align*} \n",
    "    \\mathcal{H} = \\left\\{ f_{\\theta}: \\mathbb{R} \\to \\mathbb{R} \\,|\\, f_{\\theta}(x) = \\theta_{1} x + \\theta_{2} \\right\\}, \\quad\n",
    "    (\\theta_{1}, \\theta_{2}) \\in \\mathbb{R}^{2} \\tag{1.6}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict the set of functions to this form\n",
    "def H(a, b):\n",
    "    def f(x): # <--- use this form\n",
    "        return a * x + b\n",
    "    return f\n",
    "\n",
    "# pick a specific model from the hypothesis space\n",
    "f = H(a=0, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another structures that is equally valid but not part of our hypothesis\n",
    "def another_H(a, b):\n",
    "    def f(x): # <-- \n",
    "        return b * x ** 2 + a * x + a * b\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804a36c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    üéõÔ∏è Parameter Space\n",
    "</h4>\n",
    "\n",
    "The parameter space $\\Theta$ is the set of all possible functions arising from different configurations of the parameters $\\theta \\in \\Theta$. \n",
    "\n",
    "\\begin{align*}\n",
    "    \\Theta = \\left\\{ \\theta \\in \\mathbb{R}^{p} \\,|\\, \\theta \\text{ satisfies model-specific constraints} \\right\\} \\tag{1.7}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $p$ is the number of parameters\n",
    "- $\\theta \\text{ satisfies model-specific constraints}$ represents additional constraints we place on the space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2bfa1",
   "metadata": {},
   "source": [
    "The highlight the distinction between $\\mathcal{H}$ and $\\Theta$. There are lots of different functions we could define using a set of $\\theta$, this set of functions is usually much larger than our $\\mathcal{H}$. For example, we might use $(\\theta_{1}, \\theta_{2})$ to define a linear function:\n",
    "\n",
    "\\begin{align*} \n",
    "    \\mathcal{H} = \\left\\{ f_{\\theta}: \\mathbb{R} \\to \\mathbb{R} \\,|\\, f_{\\theta}(x) = \\theta_{1} x + \\theta_{2} \\right\\} \\tag{1.8}\n",
    "\\end{align*}\n",
    "\n",
    "But we could also use it to parameterize a logistic curve, even if this is not a part of $\\mathcal{H}$:\n",
    "\n",
    "\\begin{align*} \n",
    "    \\mathcal{H} = \\left\\{ f_{\\theta}: \\mathbb{R} \\to \\mathbb{R} \\,|\\, f_{\\theta}(x) = \\frac{1}{1 + \\exp^{\\theta_{1} (x - \\theta_{2})}} \\right\\} \\tag{1.9}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89799948",
   "metadata": {},
   "source": [
    "We could also place constraints on the parameter space to further restrict different parameterizations of functions. For example, we could define specific ranges of parameters for $\\theta$ rather than any real number.\n",
    "\n",
    "\\begin{align*} \n",
    "    \\theta_{1} \\in [0, 1], \\quad\n",
    "    \\theta_{2} \\in \\left\\{ 1,2 \\right\\} \\tag{1.10}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a set of parameters to consider (doesn't have to be explicit)\n",
    "a = [0, ..., 1]\n",
    "b = [1,2]\n",
    "\n",
    "# define a hypothesis space with parameters constrained by parameter space\n",
    "def H(a, b):\n",
    "    assert a >= 0 and a <= 1, \"a must be in range [0,1]\" # <-- restricts the hypothesis space\n",
    "    assert b in [1,2], \"b must be in {1,2}\"\n",
    "    def f(x): # <--- use this form\n",
    "        return a * x + b\n",
    "    return f\n",
    "\n",
    "# invalid model based on parameter space constraints\n",
    "f = H(4.1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7270a04",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    üß† What does this all mean for our function?\n",
    "</h4>\n",
    "\n",
    "When defining our function $f$ that approximates the target function $f^{*}$. The hypothesis space $\\mathcal{H}$ and parameter space $\\Theta$ allow us to describe the structure and parameterization of our function $f$. In rare circumstances, we may be able have a strong set of prior knowledge we can use to design $\\mathcal{H}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c51d1",
   "metadata": {},
   "source": [
    "Consider the weather forecasting example: Numerical Weather Prediction (NWP) models are based on physical equations such as the Navier‚ÄìStokes equations. However, they also rely on parameterized sub-models to approximate unresolved phenomena (e.g., cloud formation, turbulence) - this represents an abstraction on the things we can't model or don't understand:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{u}_{t+\\Delta t} := \\mathcal{u}_t + \\Delta t \\left[\\underbrace{\\mathcal{N}(\\mathbf{u}, p, \\rho)}_{\\text{Navier‚ÄìStokes dynamics}} + \\underbrace{\\mathbf{P}_\\theta(\\lambda, \\dots)}_{\\text{Subgrid parameterizations}} \\right] \\tag{1.11}\n",
    "\\end{align*}\n",
    "\n",
    "In this case:\n",
    "- The structre of $f$ is mostly well-defined by prior knowledge and restricts $\\mathcal{H}$.\n",
    "- The parameters $\\theta$ are learned from empirical evidence, often through historical weather data or assimilation systems.\n",
    "\n",
    "üí¨ *Question for the audience! In your domain or problem do you have a well-known form for your function? Do you make simplified assumptions that limit your model - if so, what are they?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796af6c",
   "metadata": {},
   "source": [
    "However, for a vast number of problems, we don't even know what form the function should take...\n",
    "- What function describes a person‚Äôs reasoning process? \n",
    "- What function maps pixels in an image to its semantic meaning? \n",
    "- What function generates coherent video from a text prompt? \n",
    "\n",
    "In such scenarios, we cannot rely on prior knowledge to explicitly define or constrain the hypothesis space $\\mathcal{H}$. The structure of $f^{*}$ is unknown ‚Äî and likely inexpressible in closed-form. Instead, we turn to flexible, expressive models capable of approximating a wide range of functions. This is the motivator for architectures such as neural networks which we'll begin to explore in the next workshop.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/nextgenerationgraduatesprogram/nextgen25-mlai-workshop01/raw/main/media/notebook/Reasoning.png\" height=\"400\"/>\n",
    "    <p><em>Figure 3. What is the function for someones reasoning process?</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa7c0b",
   "metadata": {},
   "source": [
    "With this context we should discuss a fundamental source of error in machine learning, approximation error $\\epsilon_{approx}$. Approximation error represents the error between the best function we can express in the hypothesis space $f^{*}_{\\mathcal{H}}$ and the target function $f^{*}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    f^{*}_{\\mathcal{H}} = argmin_{f \\in \\mathcal{H}}\\mathcal{L}(f,f^{*}) \\tag{1.12}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, we want to design $\\mathcal{H}$ such that it can express $f^{*}$ as closely as possible, ideally $\\mathcal{H} \\subseteq \\mathcal{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d701c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52883b",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    üìâ Section 2. How do we select the best parameters?\n",
    "</h2>\n",
    "\n",
    "So to summarize where we're at. \n",
    "1. We have some function $f^{*}$ we want to approximate.\n",
    "2. We have a set of functions $f_{\\theta} \\in \\mathcal{H}$ we hypothesize may be suitable solutions which are parameterized by $\\theta \\in \\Theta$.\n",
    "\n",
    "Our next question is how do we select a parameterization $\\theta \\in \\Theta$ and then determine whether this is suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1f790",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Section 2A. Selecting a model from the hypothesis space.\n",
    "</h3>\n",
    "\n",
    "For example, we have some target function $f^{*}$ that represents a data generating process, it is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "    f^*(x) = e^{-x^{2}} \\cdot \\sin(2 \\pi \\tau x + \\varphi), \\quad \\text{for } x \\in [0, 1] \\tag{2.1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target function\n",
    "def f_target(x):\n",
    "    return np.exp(-x**2) * np.sin(2 * np.pi * 9.4 * x + 0.41)\n",
    "\n",
    "# suppose we can observe the target function perfectly (not just evaluate it)\n",
    "N = 1000\n",
    "x = np.linspace(0, 1, num=N)\n",
    "y = f_target(x)\n",
    "\n",
    "# plot the target function\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(x, y, label=\"target\")\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1e022",
   "metadata": {},
   "source": [
    "Let's define a hypothesis space (the structure) of our function and select a single function from this space using a specific set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947facf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis space\n",
    "def H(tau, phi):\n",
    "    def f(x):\n",
    "        return tau * x + phi # <-- TODO: this is probably a poor hypothesis - have a go at re-writing it\n",
    "    return f\n",
    "\n",
    "# select a function from this space\n",
    "f_approx = H(tau=1, phi=-0.5) # <-- TODO: these is probably a poor parameterization - pick some better parameters\n",
    "\n",
    "# make predictions\n",
    "y_pred = f_approx(x)\n",
    "\n",
    "# plot the target function\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(x, y, label=\"target (f*)\")\n",
    "ax.plot(x, y_pred, label=\"approx (f)\")\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cc863",
   "metadata": {},
   "source": [
    "While in this scenario since we have access to the function it's very easy to select the right hypothesis space and the select a suitable parameterization. Let's put this into a more realistic perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f7765",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    üîç Section 2B. Observing the Data Generating Processes\n",
    "</h2>\n",
    "\n",
    "While we may not known the function $f^{*}$, we can often observe its behaviour. That is, we can collect a set of data $\\mathcal{D}$ generated by the processes governing this function - usually in the form of input/output pairs $(\\mathcal{X}_{i}, \\mathcal{Y}_{i})$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{D} = \\left\\{ (\\mathcal{X}_{i}, \\mathcal{Y}_{i}) \\right\\}_{i=0}^{N} \\tag{2.2}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\mathcal{X}_{i}$ represents an observation of the input.\n",
    "- $\\mathcal{Y}_{i}$ represents an observation of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e6670",
   "metadata": {},
   "source": [
    "Observing both the input and output sides of a data generating process is rarely clean. In practice, it‚Äôs often a messy, uncertain, and biased process ‚Äî filled with various sources of noise, systematic errors, and missing information. Take the example of atmospheric forecasting. To model the state of the atmosphere, we aggregate measurements from various sources ‚Äî satellites, weather stations, radiosondes, aircraft, and remote sensors. Each of these introduces its own limitations:\n",
    "\n",
    "- $\\mathcal{X}_i$ represents the observed current or historical atmospheric state, assembled from multiple noisy sensors.\n",
    "- $\\mathcal{Y}_i$ represents the future state we hope to predict ‚Äî often inferred from models, simulations, or delayed ground-truth verification.\n",
    "\n",
    "Even with sophisticated tools, observations are rarely complete, consistent, or unbiased. Noise and bias in data don‚Äôt just affect accuracy ‚Äî they shape what kind of function we‚Äôre able to learn. Understanding the imperfections in your data is just as important as designing your model.\n",
    "\n",
    "üí¨ *Question for the audience! What sort of sources of noise exists in your input data? What about your output pairs - are they measured, simulated, annotated - what sort of sources of noise exist?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44378b",
   "metadata": {},
   "source": [
    "For example, we want to observe a function $f^{*}$, however there might be errors in where we measure and the observerations.\n",
    "\n",
    "\\begin{align*}\n",
    "    f^*(x) = e^{-x^{2}} \\cdot \\sin(2 \\pi \\tau x), \\quad \\text{for } x \\in [0, 1] \\tag{2.3}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect observation process\n",
    "def observe_perfect(f, x):\n",
    "    return f(x)\n",
    "\n",
    "# for refernce\n",
    "N = 1000\n",
    "x = np.linspace(0, 1, num=N)\n",
    "y = observe_perfect(f_target, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise observation process\n",
    "def observe_noisy(f, x):\n",
    "    # we often can't sample perfectly a point in the domain perfectly\n",
    "    x_noise = np.random.normal(0.15, 0.05, x.shape[0])\n",
    "    x_measure = x + x_noise\n",
    "\n",
    "    # we often can't measure the result perfectly\n",
    "    y_noise = np.random.normal(-0.13, 0.16, x.shape[0])\n",
    "    y_obs = f(x_measure) + y_noise\n",
    "\n",
    "    return y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a set of points we want to observe the function\n",
    "x_meas = np.random.rand((10))\n",
    "\n",
    "# noisily observe the function\n",
    "y_obs = observe_noisy(f_target, x_meas)\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "# ax.plot(x, y, label=\"target\", alpha=0.25) # <-- feel free to uncomment\n",
    "ax.scatter(x_meas, y_obs, s=6, label=\"observations\")\n",
    "ax.set_xlim(x_meas.min(), x_meas.max())\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x (where we think we sampled)\")\n",
    "ax.set_ylabel(\"y (observed response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb409d3d",
   "metadata": {},
   "source": [
    "Assuming we don‚Äôt have access to the true function $f^{*}$, it can be extremely difficult to form a hypothesis about what kind of function might have generated a dataset ‚Äî especially when the observations are sparse. From a visual standpoint, attempting to fit a curve through a handful of points can lead to wildly different interpretations depending on the assumptions we make. Lets try to sample the dataset more densely and potentially more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise observation process <-- improve this\n",
    "def observe_noisy(f, x):\n",
    "    # we often can't sample perfectly a point in the domain perfectly\n",
    "    x_noise = np.random.normal(0.15, 0.05, x.shape[0])\n",
    "    x_measure = x + x_noise\n",
    "\n",
    "    # we often can't measure the result perfectly\n",
    "    y_noise = np.random.normal(-0.13, 0.16, x.shape[0])\n",
    "    y_obs = f(x_measure) + y_noise\n",
    "\n",
    "    return y_obs\n",
    "\n",
    "# define a set of points we want to observe the function <-- sample more points\n",
    "x_meas = np.random.rand((10))\n",
    "y_obs = observe_noisy(f, x_meas)\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.scatter(x_meas, y_obs, s=6, label=\"observations\")\n",
    "ax.set_xlim(x_meas.min(), x_meas.max())\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338e1c8",
   "metadata": {},
   "source": [
    "Each observation provides a constraint on the behaviour of the function $f^{*}$ at a specific point in the input space $\\mathcal{X}$, by revealing what output it maps to $\\mathcal{Y}$. Collectively the dataset $\\mathcal{D}$ constrains the parameters of our model $f_{\\theta}$ and can be used to inform our set of hypothesis $\\mathcal{H}$.\n",
    "\n",
    "This forces us to ask whether $\\mathcal{D}$ is sufficient to capture and accurately represent the relevant features of $f^{*}$. This discussion on how sufficiently and accurately we resolve the data manifold is a crucial aspect of machine learning - in approximation theory e.g. polynomial interpolation, error bounds of the function approximation often depend on the maximum spacing between samples:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\epsilon \\propto \\delta^{k} \\cdot || f^{(k)} ||_{\\infty}, \\quad \n",
    "    \\delta = max_{i}(\\mathcal{X}_{i+1} - \\mathcal{X}_{i}) \\tag{2.4}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\epsilon$ is the error bound\n",
    "- $f^{(k)}$ is the $k$-th derivative of a function $f$\n",
    "- $\\delta$ is the largest distance between points\n",
    "\n",
    "Where a functions value changes rapidly (high $f^{(k)}$) to make sure we can constrain our function (minimize $\\epsilon$) we need to sample the points more densely around this area of change (minimize $\\delta$).\n",
    "\n",
    "üí¨ *Question for the audience! Does your dataset accurately resolve your data manifold? How do you determine this when it comes to extremely high-dimensional and abstract data?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36302e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a suitable parameterization <-- vary the parameterization\n",
    "f_approx = H(tau=1, phi=1)\n",
    "\n",
    "# get our predictions\n",
    "y_pred = f_approx(x_meas)\n",
    "\n",
    "# plot the target function\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.scatter(x_meas, y_obs, s=6, label=\"target\")\n",
    "ax.scatter(x_meas, y_pred, s=6, label=\"predicted\")\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c538e",
   "metadata": {},
   "source": [
    "As you're going through this manual optimization process it's good to notice that you're relying on your observations $\\mathcal{D}$ to constrain $f_{\\theta}$ - this is more or less the process of learning - constraining your model using evidence. However, this process of fitting functions by hand isn't a particularly rigorous approach - what does it mean for your parameterization to be a good fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522dbec",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    üéØ Section 2C. Evaluating the fit of our function\n",
    "</h3>\n",
    "\n",
    "During the learning process, we need a way to measure how well our function $f_{\\theta}$ performs to guide our parameterization $\\theta \\in \\Theta$. In an ideal world, we would compare our approximation $f$ directly to the target function $f^{*}$ (which we don't have access to):\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}(f_\\theta, f^{*}) = ... \\tag{2.5}\n",
    "\\end{align*}\n",
    "\n",
    "However, we do have observations of $f^{*}$ as represented by $\\mathcal{D}$. We can instead compare how well our function $f$ evaluated at a point $\\mathcal{X}_{i}$ predicts the observations $\\mathcal{Y}_{i}$.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{\\mathcal{Y}}_i = f_\\theta(\\mathcal{X}_i) \\tag{2.6}\n",
    "\\end{align*}\n",
    "\n",
    "By comparing $\\hat{\\mathcal{Y}}_i$ with the observations $\\mathcal{Y}_{i}$ we can compute an empirical approximation of the true error as defined by the observations:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}(\\mathcal{D}, f_\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(f_\\theta(x_i), y_i\\right) \\tag{2.7}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\ell(\\cdot, \\cdot)$ is a pointwise loss function, such as the mean squared error $(\\hat{\\mathcal{Y}}_i - \\mathcal{Y}_{i})^{2}$\n",
    "- $\\mathcal{L}$ is the average loss over the dataset $\\mathcal{D}$.\n",
    "\n",
    "This loss value $\\mathcal{L}$ provides feedback about how good our parameterization $f_{\\theta}$ is, based on the evidence $\\mathcal{D}$. There are numerous ways to define the pointwise loss function $\\ell$, each reflecting different assumptions, goals, or properties of the task and this also has implications for the learning process as we'll see. Lets see how we could implement an example below:\n",
    "\n",
    "üí¨ *Question for the audience! What loss functions have you used before? Have you used a different loss function and seen a significant difference in model behaviour?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a67131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define a pointwise loss function\n",
    "def pointwise_mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    mean squared error (MSE)\n",
    "    \"\"\"\n",
    "    return (y_hat - y) ** 2\n",
    "\n",
    "# lets define a function that computes the average loss across a dataset\n",
    "def compute_dataset_loss(f, x, y, loss_fn):\n",
    "    # compute predictions\n",
    "    y_hat = f(x)\n",
    "\n",
    "    # number of samples\n",
    "    N = y.shape[0]\n",
    "\n",
    "    # accumulate loss across samples\n",
    "    L = 0\n",
    "    for i in range(N):\n",
    "        L += loss_fn(y[i], y_hat[i])\n",
    "\n",
    "    # compute average loss\n",
    "    L = (1/N) * L\n",
    "\n",
    "    return float(L)\n",
    "\n",
    "# lets compute the average mse loss across the dataset for our function\n",
    "L_D = compute_dataset_loss(f, x_meas, y_obs, pointwise_mse_loss)\n",
    "print(f\"L_D = {L_D:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f23ecd",
   "metadata": {},
   "source": [
    "Let's have a go at another loss and evaluate it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_pointwise_loss(y, y_hat):\n",
    "    ... # <-- your loss here\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "L_D = compute_dataset_loss(f, x_meas, y_obs, your_pointwise_loss)\n",
    "print(f\"L_D = {L_D:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c4db4",
   "metadata": {},
   "source": [
    "<h3>üèûÔ∏è Section 2D. The Loss Landscape</h3>\n",
    "\n",
    "As a result of not being able to use $f^{*}$ directly we had to rely on a dataset $\\mathcal{D}$ to evaluate our function $f_{\\theta}$ - hence we can consider our loss as a function of the dataset $\\mathcal{D}$ and parameters $\\theta$, with some profound consequences. Different datasets and loss functions define different loss functions: i.e. surfaces over parameter space that determine how good each model is. These landscapes, in turn, define which models are viable.\n",
    "\n",
    "Formally, we define the empirical-risk minimiser $\\hat{f}$ as the function in our hypothesis space $\\mathcal{H}$ that best fits the training data $\\mathcal{D}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{f} = argmin_{f_{\\theta} \\in \\mathcal{H}} \\mathcal{L}(\\mathcal{D}, f_\\theta) \\tag{2.8}\n",
    "\\end{align*}\n",
    "\n",
    "This leads to another fundamental source of error in machine learning, the generalization error, $\\epsilon_{gen}$. The generalization error represents the error between $\\hat{f}$ and $f^{*}$, in essense the error as a result of the dataset constraining how well we can learn our function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1a2f5",
   "metadata": {},
   "source": [
    "We can explore how our loss, defined by $\\mathcal{L}(\\mathcal{D}, f_\\theta)$, varies as we modify the parameters $\\theta$ of our funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets consider a single parameter\n",
    "theta_0 = np.linspace(-10, 10, num=1000)\n",
    "\n",
    "# lets compute the loss (error across the dataset) for each function defined by the parameter\n",
    "losses = np.zeros_like(theta_0)\n",
    "for i in range(theta_0.shape[0]):\n",
    "    # select a function from the hypothesis space\n",
    "    f_approx = H(tau=theta_0[i], phi=1)\n",
    "\n",
    "    # perform predictions based on dataset\n",
    "    y_pred = f_approx(x_meas)\n",
    "\n",
    "    # compute loss across dataset\n",
    "    losses[i] = compute_dataset_loss(f_approx, x, y_obs, pointwise_mse_loss)\n",
    "\n",
    "\n",
    "# lets plot the result\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(theta_0, losses)\n",
    "ax.set_xlim(theta_0.min(), theta_0.max())\n",
    "ax.set_xlabel(\"theta_0\")\n",
    "ax.set_ylabel(\"Dataset Loss (MSE)\")\n",
    "ax.set_title(f\"Loss Landscape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96259049",
   "metadata": {},
   "source": [
    "We can extend this idea to two dimensions and visualize how the loss functions varies as a function of two different parameter values. We can think of this as a slice of the full loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408fc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets consider a single parameter\n",
    "theta_0 = np.linspace(-2.5, 2.5, num=100)\n",
    "theta_1 = np.linspace(-2.5, 2.5, num=100)\n",
    "T0, T1 = np.meshgrid(theta_0, theta_1, indexing=\"ij\")\n",
    "\n",
    "# lets compute the loss (error across the dataset) for each function defined by the parameter\n",
    "losses = np.zeros_like(T0)\n",
    "for i in range(T0.shape[0]):\n",
    "    for j in range(T1.shape[0]):\n",
    "        # select a function from the hypothesis space\n",
    "        f_approx = H(tau=T0[i,j], phi=T1[i,j])\n",
    "\n",
    "        # compute loss across dataset\n",
    "        losses[i,j] = compute_dataset_loss(f_approx, x_meas, y_obs, pointwise_mse_loss)\n",
    "        \n",
    "\n",
    "# lets plot the result\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "s = ax.plot_surface(T0, T1, losses, cmap='viridis')\n",
    "ax.set_xlim(theta_0.min(), theta_0.max())\n",
    "ax.set_ylim(theta_1.min(), theta_1.max())\n",
    "ax.set_xlabel(\"theta_0 (tau)\")\n",
    "ax.set_ylabel(\"theta_1 (phi)\")\n",
    "ax.set_zlabel(\"Dataset Loss (MSE)\")\n",
    "ax.set_title(f\"Loss Landscape\")\n",
    "\n",
    "# as a contour plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "contour = ax.contourf(T0, T1, losses, levels=50, cmap='viridis')\n",
    "ax.set_xlim(theta_0.min(), theta_0.max())\n",
    "ax.set_ylim(theta_1.min(), theta_1.max())\n",
    "ax.set_xlabel(\"theta_0 (tau)\")\n",
    "ax.set_ylabel(\"theta_1 (phi)\")\n",
    "ax.set_title(f\"Loss Landscape (Top-down View)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2ba90",
   "metadata": {},
   "source": [
    "We can view our function as being at a specific point in this space defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a contour plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "contour = ax.contourf(T0, T1, losses, levels=50, cmap='viridis')\n",
    "ax.set_xlim(theta_0.min(), theta_0.max())\n",
    "ax.set_ylim(theta_1.min(), theta_1.max())\n",
    "ax.set_xlabel(\"theta_0 (tau)\")\n",
    "ax.set_ylabel(\"theta_1 (phi)\")\n",
    "ax.set_title(f\"Loss Landscape (Top-down View)\")\n",
    "\n",
    "# plot our function in parameter space\n",
    "tau, phi = -1.4, 1.9\n",
    "ax.scatter(tau, phi, s=64, marker=\"x\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66764c",
   "metadata": {},
   "source": [
    "We can't easily visualise spaces with more than three dimensions, but the idea of a loss landscape is still a powerful way to think about learning. This landscape is a geometric surface defined by the loss function, where low points (minima) represent good models (low loss) and high points (maxima) represent poor models (high loss). As we change the parameters of our model, we move around on this landscape.\n",
    "\n",
    "The loss landscape is a geometric object that encodes the structure of the learning problem - linking together the dataset, model, and eventually the optimization procedure. Understanding its shape helps us reason about trainability, sensitivty of the model to initialization, and the nature of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160140c",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Section 3. Optimization - The Learning in Machine Learing \n",
    "</h2>\n",
    "\n",
    "In machine learning, once we've chosen a hypothesis $\\mathcal{H}$ space and defined a loss function $\\mathcal{L}(f_\\theta(x), \\mathcal{D})$, we need a way to find the best parameters we can $\\theta \\in \\Theta$ that make our predictions $f_\\theta(x)$ as close as possible to the true outputs $y$ in the dataset - i.e. minimize that loss. This is essentially an optimization problem where we're aiming to explore the $\\mathcal{H}$ - as we've seen we can view this is varying $\\theta$ to minimize $\\mathcal{L}(f_\\theta(x), \\mathcal{D})$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta^* = \\arg\\min_\\theta \\mathcal{L}(f_\\theta(x), y) \\tag{3.1}\n",
    "\\end{align*}\n",
    "\n",
    "While there are lots of different techniques to perform optimization such as evolutionary algorithms, we've been building up the reasoning towards using another algoritmh - called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7e2c7",
   "metadata": {},
   "source": [
    "<h3>Section 3A. Gradient Descent</h3>\n",
    "\n",
    "Gradient descent is one of the most widely used optimization algorithms in machine learning. It‚Äôs an iterative method that updates model parameters in the direction that most rapidly decreases the loss. You can imagine the loss surface as a 3D landscape with hills, valleys, and a lowest point ‚Äî the global minimum. Gradient descent uses the gradient (a vector indicating the slope of the loss surface at a point) to determine the direction of steepest descent, and takes a step in that direction.\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(f_\\theta(x), y) \\tag{3.2}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\nabla_\\theta \\mathcal{L}$ is the gradient of the loss w.r.t. parameters.\n",
    "- $\\eta$ is the learning rate - the size of our step.\n",
    "\n",
    "When the loss function is convex ‚Äî such as in linear regression with mean squared error ‚Äî gradient descent is guaranteed to find the global minimum. For non-convex loss surfaces, like those arising in deep neural networks, it may converge to a local minimum or saddle point. However, in practice, gradient descent often still finds solutions that generalize well.\n",
    "\n",
    "Gradient descent forms the computational backbone of most modern machine learning algorithms, from simple linear models to large-scale deep learning systems.\n",
    "\n",
    "So how do we compute and apply the gradient? Enter calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2696d",
   "metadata": {},
   "source": [
    "So how do we compute the gradient of the loss landscape $\\nabla_\\theta \\mathcal{L}$ with respect to $\\theta$?\n",
    "\n",
    "We are minimizing the **mean squared error (MSE)** loss:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\tau, \\varphi) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( f_\\theta(x_i) - y_i \\right)^2 \\tag{3.2}\n",
    "\\end{align*}\n",
    "\n",
    "Where the model (our hypothesis function) is:\n",
    "\n",
    "\\begin{align*}\n",
    "f_\\theta(x) = e^{-x^2} \\cdot \\sin(2\\pi \\tau x + \\varphi) \\tag{3.3}\n",
    "\\end{align*}\n",
    "\n",
    "Our goal is to compute:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\tau} \\quad \\text{and} \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\varphi} \\tag{3.4}\n",
    "\\end{align*}\n",
    "\n",
    "Let‚Äôs define the output of the model for a single input, then the loss becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L} = \\frac{1}{n} \\sum (f_\\theta(x_i) - y_i)^2 \\tag{3.5}\n",
    "\\end{align*}\n",
    "\n",
    "The chain rule allows us to decompose the loss derivative into the product of:\n",
    "- The derivative of the loss with respect to the model output (error),\n",
    "- And the derivative of the model output with respect to the parameter.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\tau} \n",
    "= \\frac{\\partial \\mathcal{L}}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\tau} \n",
    "= \\frac{1}{n} \\sum 2(f_\\theta(x_i) - y_i) \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\tau} \\tag{3.6}\n",
    "\\end{align*}\n",
    "\n",
    "Now differentiate the model function with respect to $\\tau$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f_\\theta(x_i)}{\\partial \\tau}\n",
    "= \\frac{d}{d\\tau} \\left[ e^{-x_i^2} \\cdot \\sin(2\\pi \\tau x_i + \\varphi) \\right]\n",
    "= e^{-x_i^2} \\cdot \\cos(2\\pi \\tau x_i + \\varphi) \\cdot 2\\pi x_i \\tag{3.7}\n",
    "\\end{align*}\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\tau}\n",
    "= \\frac{2}{n} \\sum (f_\\theta(x_i) - y_i) \\cdot e^{-x_i^2} \\cdot \\cos(2\\pi \\tau x_i + \\varphi) \\cdot 2\\pi x_i \\tag{3.8}\n",
    "\\end{align*}\n",
    "\n",
    "...Follow the same process for $\\varphi$...\n",
    "\n",
    "\n",
    "\n",
    "This lets us efficiently compute how changes in parameters influence the overall loss, and is the mechanism used in frameworks like PyTorch and TensorFlow during automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f866cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute the derivatives of our function\n",
    "def df_dtau(x, tau, phi):\n",
    "    return 2 * np.pi * x * np.exp(-x**2) * np.cos(2 * np.pi * tau * x + phi)\n",
    "\n",
    "def df_dphi(x, tau, phi):\n",
    "    return np.exp(-x**2) * np.cos(2 * np.pi * tau * x + phi)\n",
    "\n",
    "\n",
    "# and compute the derivative of the loss wrt to our function\n",
    "def grad_mse_tau_phi(x, y, tau, phi):\n",
    "    y_pred = H(tau, phi)(x)\n",
    "    error = y_pred - y\n",
    "\n",
    "    dL_dtau = 2 * np.mean(error * df_dtau(x, tau, phi))\n",
    "    dL_dphi = 2 * np.mean(error * df_dphi(x, tau, phi))\n",
    "\n",
    "    return dL_dtau, dL_dphi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets setup a grid of parameters\n",
    "theta_0 = np.linspace(-2.5, 2.5, num=100)  # tau\n",
    "theta_1 = np.linspace(-2.5, 2.5, num=100)  # phi\n",
    "T0, T1 = np.meshgrid(theta_0, theta_1, indexing=\"ij\")\n",
    "\n",
    "# lets compute the loss at each point of the grid (for visualization)\n",
    "losses = np.zeros_like(T0)\n",
    "for i in range(T0.shape[0]):\n",
    "    for j in range(T0.shape[1]):\n",
    "        _f = H(tau=T0[i, j], phi=T1[i, j])\n",
    "        losses[i, j] = compute_dataset_loss(_f, x_meas, y_obs, pointwise_mse_loss)\n",
    "\n",
    "# 1. define the parameter values\n",
    "tau, phi = 0.2, 1.9\n",
    "\n",
    "# 2. compute the gradient of the loss with respect to each parameter - \\nalba_{theta}\n",
    "dL_dtau, dL_dphi = grad_mse_tau_phi(x_meas, y_obs, tau, phi)\n",
    "\n",
    "# 3.  scale the gradient by a step size\n",
    "lr = 1.00\n",
    "dL_dtau = lr * dL_dtau\n",
    "dL_dphi = lr * dL_dphi\n",
    "\n",
    "# plot contour and gradient vector\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(T0, T1, losses, levels=50, cmap='viridis')\n",
    "ax.set_xlim(T0.min(), T0.max())\n",
    "ax.set_ylim(T1.min(), T1.max())\n",
    "ax.set_xlabel(\"tau\")\n",
    "ax.set_ylabel(\"phi\")\n",
    "ax.set_title(\"Loss Landscape with Gradient Vector\")\n",
    "ax.quiver(tau, phi, dL_dtau, dL_dphi, angles='xy', scale_units='xy', scale=1, color='red', width=0.01)\n",
    "ax.plot(tau, phi, 'ro', label=\"Start Point\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aedcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_traj(tau_init, phi_init, x, y, lr, steps):\n",
    "    tau, phi = tau_init, phi_init\n",
    "    history = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # define model and compute error\n",
    "        f_approx = H(tau, phi)\n",
    "        y_pred = f_approx(x)\n",
    "        error = y_pred - y\n",
    "\n",
    "        # compute gradients using known partial derivatives\n",
    "        grad_tau = 2 * np.mean(error * df_dtau(x, tau, phi))\n",
    "        grad_phi = 2 * np.mean(error * df_dphi(x, tau, phi))\n",
    "\n",
    "        # store history before update\n",
    "        loss = np.mean((y - y_pred)**2)\n",
    "        history.append((tau, phi, loss))\n",
    "\n",
    "        # gradient descent update\n",
    "        tau -= lr * grad_tau\n",
    "        phi -= lr * grad_phi\n",
    "\n",
    "    return np.array(history)\n",
    "\n",
    "# Run gradient descent\n",
    "trajectory = gradient_descent_traj(tau_init=1.0, phi_init=0.0, x=x_meas, y=y_obs, lr=0.1, steps=50)\n",
    "\n",
    "# Plot loss over iterations\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(trajectory[:, 2], label=\"Loss over Iterations\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss (MSE)\")\n",
    "ax.set_title(\"Loss Curve\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Plot the path on the loss landscape\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(T0, T1, losses, levels=50, cmap='viridis')\n",
    "ax.set_xlim(T0.min(), T0.max())\n",
    "ax.set_ylim(T1.min(), T1.max())\n",
    "ax.set_xlabel(\"tau\")\n",
    "ax.set_ylabel(\"phi\")\n",
    "ax.set_title(\"Gradient Descent Path on Loss Landscape\")\n",
    "\n",
    "# Path\n",
    "ax.plot(trajectory[0, 0], trajectory[0, 1], 'rx', label=\"Initial Point\")\n",
    "ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-', alpha=0.25, label=\"Gradient Descent Path\")\n",
    "ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', label=\"Final Point\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9577f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized model\n",
    "final_tau, final_phi = tuple(trajectory[-1,:2].tolist())\n",
    "f_tilde = H(final_tau, final_phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbd007",
   "metadata": {},
   "source": [
    "We refer to the model with the set of parameters determined by the optimization process as $\\tilde{f}$.\n",
    "\n",
    "This leads to another fundamental source of error in machine learning, the optimization error, $\\epsilon_{opt}$. The optimization error error represents the error between $\\hat{f}$ (best possible model your hypothesis space could express based on the data) and $\\tilde{f}$ (the model your optimization algorithm actually found), in essense the error as a result of the imperfect optimization process. Even when the hypothesis space $\\mathcal{H}$ is expressive enough to represent a good approximation of the true function, you may not reach that optimum during training. This happens due to:\n",
    "- Poor initialization: Your starting point in parameter space may be far from the optimum.\n",
    "- Learning rate issues: If your learning rate is too high, you may overshoot the minimum. If too low, training may stagnate.\n",
    "- Early stopping: Optimization may be halted before convergence due to computational limits or regularization strategies.\n",
    "- Local minima or saddle points: In non-convex landscapes, the optimizer may get trapped in regions that are suboptimal.\n",
    "- Noisy or stochastic gradients: In mini-batch training, variance in gradient estimates can lead to imperfect updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760f668",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3508391a",
   "metadata": {},
   "source": [
    "<h2>üß† Workshop Summary: What Have We Learned?</h2>\n",
    "\n",
    "Over the course of this workshop, we‚Äôve taken a step back from just \"using machine learning models\" and instead focused on building a deeper understanding of how learning works ‚Äî and why. Whilst this first workshop was very theory heavy - we're going to revisit and reinforce some of these concepts throughout the next set of workshops - hopefully this provides some good intuition for the future.\n",
    "\n",
    "\n",
    "<h3>üîÅ 1. Machine Learning is About Learning Functions</h3>\n",
    "\n",
    "We reframed machine learning as the task of approximate an unknown function $f^*$ that maps inputs to outputs:\n",
    "\\begin{align*}\n",
    "    f^*: \\mathcal{X} \\rightarrow \\mathcal{Y}\n",
    "\\end{align*}\n",
    "\n",
    "Since we rarely know $f^*$ directly, we observe data $\\mathcal{D}$ ‚Äî noisy samples of its behavior ‚Äî and attempt to learn an approximation $f_\\theta$ that generalizes well.\n",
    "\n",
    "<h3>üìê 2. We Define a Hypothesis Space</h3>\n",
    "\n",
    "We choose a hypothesis space $\\mathcal{H}$: the family of functions we‚Äôre willing to consider. This reflects our assumptions and modeling choices ‚Äî linear functions, neural nets, polynomials, etc.\n",
    "\n",
    "\n",
    "<h3>üéØ 3. We Use Data to Constrain the Hypothesis Space</h3>\n",
    "\n",
    "Each observation in $\\mathcal{D}$ acts as a constraint on what $f^*$ might be. The dataset, function, and loss function defines a surface over parameter space, and training means searching this surface for a good minimum.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}(\\theta) = \\frac{1}{n} \\sum \\mathcal{L}(f_\\theta(x_i), y_i)\n",
    "\\end{align*}\n",
    "\n",
    "<h3>üîΩ 4. We Minimize Loss via Optimization</h3>\n",
    "\n",
    "We explored gradient descent as a method to minimize the loss. We visualized how the landscape behaves, how gradients flow, and how descent leads to improvement ‚Äî but not always perfection.\n",
    "\n",
    "<h3>‚ùó 5. We Encounter Fundamental Errors</h3>\n",
    "\n",
    "We introduced three core sources of error in machine learning:\n",
    "- Approximation error: when our model can‚Äôt express the true function.\n",
    "- Optimization error: our optimizer didn‚Äôt find the best possible model.\n",
    "- Generalization error: we didn‚Äôt see enough data to make a reliable estimate.\n",
    "\n",
    "<h3>üß≠ Final Takeaway</h3>\n",
    "\n",
    "> **Machine learning isn‚Äôt just about tools or code ‚Äî it‚Äôs about assumptions, constraints, and learning from evidence.**\n",
    "\n",
    "Everything we do ‚Äî from choosing a model to training and evaluation ‚Äî reflects this mindset:  \n",
    "We are learning a function, from data, under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf148a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695d2c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
